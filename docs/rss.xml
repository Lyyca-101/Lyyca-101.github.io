<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Lyyca-101's fast Blog</title><link>https://Lyyca-101.github.io</link><description>fast blog site</description><copyright>Lyyca-101's fast Blog</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>https://Lyyca-101.github.io</link></image><lastBuildDate>Tue, 19 Nov 2024 13:31:53 +0000</lastBuildDate><managingEditor>Lyyca-101's fast Blog</managingEditor><ttl>60</ttl><webMaster>Lyyca-101's fast Blog</webMaster><item><title>[FAST2016]The Composite-file File System: Decoupling the One-to-One Mapping of Files and Metadata for Better Performance</title><link>https://Lyyca-101.github.io/post/%5BFAST2016%5DThe%20Composite-file%20File%20System-%20Decoupling%20the%20One-to-One%20Mapping%20of%20Files%20and%20Metadata%20for%20Better%20Performance.html</link><description>&lt;h2&gt;[FAST2016]The Composite-file File System:&#13;
Decoupling the One-to-One Mapping&#13;
of Files and Metadata for Better Performance&lt;/h2&gt;&#13;
&#13;
现状：&#13;
&#13;
- 每个逻辑文件都被映射到它自己的一个物理元数据以及数据表示&#13;
- 也就是一个1-to-1的文件-元数据映射&#13;
- 对很多的文件系统机制而言，这是一个自然的粒度(granularity)&#13;
&#13;
根据作者的观察(observation)，这种1-to-1的映射可能错失了很多优化的机会，主要的观察有四个：&#13;
&#13;
- 对小文件的频繁访问：作者的桌面文件系统分析表明，&gt;80%的访问落在大小&lt;32B的小文件上，而在对这些盘上的小文件数据的访问时间中，对元数据的访问时间占了40%，因此，减少这部分的访问开销将带来巨大的性能提升&#13;
- 冗余的元数据信息：传统文件关联的元数据，会记录文件的数据块位置、访问权限等，许多文件其实拥有相同或类似的文件属性，对于一些文件属性(访问权限、所有者等)，其可能情况是明显有限的，因此会有大量的文件在这些属性上相似，一个研究表明，对一个工作站的元数据进行压缩，压缩率达到75%(per-inode,128B--&gt;15-20B)，有很多机会去减少冗余的元数据，从而提高空间利用率&#13;
- 成组访问文件：很多研究表明，文件倾向于被一起访问，但是仅仅将文件分组并不能得到性能提升，应为识别可分租的文件以及将文件分组会带来额外的开销&#13;
- 预取的局限：尽管预取是一个有效的优化手段，但是提取文件数据-文件元数据的分离动作会导致极大的额外开销，比如，32个小文件和1个和32个小文件一样大的单文件，访问前者的延迟会比后者高50%，哪怕是在已经被缓存的情况下&#13;
&#13;
那么，如何减少对元数据的额外访问开销，减少相似元数据占用的空间，低开销地成组访问文件，作者提出一个idea，同时地去解决这些问题，即：&#13;
&#13;
- 将一起访问的小文件组合在一起，解耦1-to-1的文件-元数据映射，构造many-to-1的文件-元数据映射&#13;
&#13;
作者的设计：。</description><guid isPermaLink="true">https://Lyyca-101.github.io/post/%5BFAST2016%5DThe%20Composite-file%20File%20System-%20Decoupling%20the%20One-to-One%20Mapping%20of%20Files%20and%20Metadata%20for%20Better%20Performance.html</guid><pubDate>Tue, 19 Nov 2024 13:31:31 +0000</pubDate></item><item><title>[XV6 Lab1]TAB completion and command history</title><link>https://Lyyca-101.github.io/post/%5BXV6%20Lab1%5DTAB%20completion%20and%20command%20history.html</link><description>&lt;h1 &gt;关于XV6 Lab1的shell tab completion challenge&lt;/h1&gt;&#13;
首先得想办法让terminal能够不回显tab&lt;br&gt;&#13;
其次是参考GNU readline library，其中有非常成熟的命令补全以及命令历史记录的实现&lt;br&gt;&#13;
实际上，很多的shell、一些工具的命令行模式，都使用了这个库&lt;br&gt;&#13;
&lt;h2&gt;关于shell tab completion&lt;/h2&gt;&#13;
目前的terminal driver似乎只能支持行缓冲模式，即，让read返回直到用户键入回车前的所有字符&lt;br&gt;&#13;
参照readline的实现，我们现在想要一个这样的模式：每次键入一个按键，read就马上返回这个按键对应的编码值&lt;br&gt;&#13;
这样一来，针对返回的一般字母数字以及回车，我们将其回显，而tab则不回显，返回的字符用一个buffer缓冲&lt;br&gt;&#13;
当返回的字符是tab时，shell可以进行completion尝试补全缓冲区的字符串，这里可以基于当前目录的文件名进行补全。</description><guid isPermaLink="true">https://Lyyca-101.github.io/post/%5BXV6%20Lab1%5DTAB%20completion%20and%20command%20history.html</guid><pubDate>Mon, 18 Nov 2024 13:14:05 +0000</pubDate></item><item><title>Pelican:A Building Block for Exascale Cold Data Storage[OSDI2014]</title><link>https://Lyyca-101.github.io/post/Pelican-A%20Building%20Block%20for%20Exascale%20Cold%20Data%20Storage%5BOSDI2014%5D.html</link><description>&lt;h2&gt;[OSDI2014]Pelican[COLON] A Building Block for Exascale Cold Data Storage&lt;/h2&gt;&#13;
&#13;
建议去USENIX看看这篇paper的presentation和slides&#13;
&#13;
对'cold data'的描述是云存储中很少访问的数据，这类数据占了相当大的比例&#13;
&#13;
- pelican是面向EB级冷数据存储、基于磁盘的机架级存储单元&#13;
- 将访问不频繁的数据存储在早期的基于磁盘的云存储中，是昂贵的，因为这些磁盘的配置基于热数据的工作负载，使用的配置提供低访问延迟以达到顶峰性能(peak performance，这会导致机架的上行链路带宽被完全充满，完全利用)，为了达到这一点，会有大量的供电、冷却和服务器支持，这带来了很大的成本开销，，如何为这类数据设计高性价比的存储成为一个挑战&#13;
- pelican的原型能够存储超过5PB的原始数据(raw data)&#13;
- pelican被设计为仅支持冷数据工作负载，每PB提供的峰值持续读取速度为1GB/PB/s&#13;
- 通过精心设计的软件栈使得pelican能够在仅有8%磁盘工作的时候仍能够满足冷数据工作负载的需求&#13;
- right-provisioning，一个刚好能够满足冷数据工作负载(如何分析负载得到的配置)的硬件配置&#13;
  - 根据冷数据的访问模式(写一次，很少读write once rarely read)，不需要所有的磁盘都同时工作来提供高性能，那么为了高性能而设置的大量供电、冷却、服务器设施就是不必要的了&#13;
  - 大量的电力供应、冷却机构、服务器被移除，使得这些硬件只占用机架极少一部分的空间，从而使得52U的机架能够放置1152个磁盘&#13;
  - 只有2个server进行管理&#13;
  - 上行带宽为4*10Gbps&#13;
  - 磁盘也是Archival级别的，不是commodity级别，这种磁盘对spin-up延迟和容量作了优化&#13;
  - right-provisioning削减了大量的硬件资源，这导致的结果是，并非所有的磁盘都能在任何时候启动&#13;
  - 实际上，冷却机构只能让8%磁盘同时旋转，如果超过，就会导致有的磁盘过热从而影响使用寿命&#13;
  - 存储系统的配置下，冷却机构只能让96个磁盘同时转动(一共1152个磁盘，机架大小为52U，~22disks/U)&#13;
  - right-provisioning虽然减少了成本，增加了存储密度，但是硬件资源的减少带来了明显的资源约束问题，作者将会采用软件的方法进行解决&#13;
&#13;
- pelican将大量的磁盘分成磁盘组管理，对于1152个磁盘，分成48个磁盘组进行管理，组内的磁盘接受同样的磁盘物理约束(电力、带宽、冷却、故障域等)，在同一个组的磁盘会同时启动&#13;
- pelican通过分组进行数据布局和IO调度&#13;
&#13;
第2节提供Pelican硬件的概述，并描述数据布局和I/O调度器，但在讨论硬件结构之后、数据布局之前，作者首先详细定义了资源约束，然后再将layout和I/O schedule是如何解决资源约束问题的&#13;
&#13;
资源约束：&#13;
&#13;
- 对活动磁盘组的约束&#13;
- 定义了资源域(resource domain)，有四种：power cooling vibration bandwidth&#13;
- paper里的图为了简单展示，没有呈现vibration domain和bandwidth domain&#13;
- power domain：16个中的2个disks可以活动&#13;
- cooling domain: 12个中的1个disk可以活动&#13;
- vibration domain：2个中的1个disk可以活动&#13;
&#13;
在这样的资源约束下，数据布局，或者说数据放置的目标是最大化请求的并发度，也就是说，对于到达的请求，让尽可能多的请求能够被同时服务，冲突感知的数据放置&#13;
&#13;
- 数据被按blob组织，一个blob被存入pelican时，会使用纠删码进行编码，并且blob会被均分成多个片段(fragment)存到一组磁盘中(stripe)&#13;
- 这些片段所在的一组磁盘被要求能够同时活动，以最大化IO性能&#13;
- 让任意两组磁盘组同时活动，在硬件资源丰富的系统，是没有问题的&#13;
- 但是在pelican的right-provisioning的系统中，由于硬件资源紧缺，两组磁盘组可能会发生资源上的竞争(供电不够用、冷却不够用等)，比如这两组磁盘中的某两个disk的资源域重叠(在同一列的disks，发生冷却资源的竞争)&#13;
- 因此需要最小化资源冲突的概率&#13;
- 考虑一个blob经过编码产生n段放置到n个磁盘，如果随机放置，那么对于下一个要放置到n个磁盘的blob，发生冲突的概率符合O(n^2^)&#13;
- 让冲突在组与组之间发生&#13;
- 组与组之间要么完全冲突，要么完全不冲突&#13;
- 完全冲突的组组成的集合，称为class&#13;
- 静态地将磁盘分成组，通过按对角线分(paper有图)，这样能保证组内的每个disk都在单独的power domain和cooling domain，这样组内的disk不会发生资源冲突，只有可能组与组之间发生冲突&#13;
- 这使得冲突的概率变成符合O(n)&#13;
- pelican设置1个组有24个disks，对于1152个磁盘，有48个组，形成4个class，每个class有12个组，在不同class的组不会发生资源冲突，所以系统的并发度是4，也就是说，如果有4个对不同class上的请求，它们能够被同时服务&#13;
- 需要注意，paper声称pelican只是EB级冷数据存储的building block，这意味着实际部署的时候，必定是部署大量的pelican机架，而不是只有一台。</description><guid isPermaLink="true">https://Lyyca-101.github.io/post/Pelican-A%20Building%20Block%20for%20Exascale%20Cold%20Data%20Storage%5BOSDI2014%5D.html</guid><pubDate>Wed, 06 Nov 2024 09:04:17 +0000</pubDate></item><item><title>First shot</title><link>https://Lyyca-101.github.io/post/First%20shot.html</link><description>Write something to see if it works functionally.&#13;
seems it can be modified.。</description><guid isPermaLink="true">https://Lyyca-101.github.io/post/First%20shot.html</guid><pubDate>Mon, 04 Nov 2024 11:45:22 +0000</pubDate></item></channel></rss>